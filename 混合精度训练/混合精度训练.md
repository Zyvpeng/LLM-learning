# 混合精度训练

## 1. 数据存储方式

在一般的深度学习任务中，一般使用float也就是float32（单精度）浮点表示。

但是也有一些GPU上支持半精度运算float16

具体的bitmap如下

- 单精度

  ![image-20250127221347259](https://s2.loli.net/2025/01/27/TFXq5jr6zy3PNWf.png)

- 半精度

  - fp16  8bit指数 7bit小数 

  - fp16  5bit指数 10bit小数

    ![image-20250127221502347](https://s2.loli.net/2025/01/27/nPcQFYw3tohldvC.png)

==下文的分析都以fp16为例==

![image-20250127221645103](https://s2.loli.net/2025/01/27/QZjKDVGiU24BJkg.png)



## 2. 半精度训练

### 2.1半精度的优势

- 内存占用减小：模型内存占用直接除以二，activation、grads也都大小减半
  - 可以用更大的batchsize
  - 多卡训练时，通信量大大减少，加速数据流通
- 计算更快
  - 半精度吞吐量可以达到单精度的2-8倍

### 2.2半精度面临的问题

#### 2.2.1数据溢出

fp16的有效范围是：2的-24次幂到65504，最大的问题是下溢出

因为参数更新需要计算lr*grads，在训练后期，这个数会很小很小，很有可能下溢出

![image-20250127222310364](https://s2.loli.net/2025/01/27/OXko9PVyuIQWBih.png)



#### 2.2.2舍入误差

无论是半精度还是单精度，它们都不是完全连续的，而是有一个最小间隔的

fp16的最小gap如下图所示

![image-20250127222451175](https://s2.loli.net/2025/01/27/U3StynCQDjHZ4f5.png)

这个例子可以很好的说明问题

![image-20250127222520433](https://s2.loli.net/2025/01/27/MVZSipNqw5b1jKP.png)

## 3. 混合精度训练

混合精度训练的目的就是在保留半精度优点的同时解决==溢出==和==舍入误差==这两个主要问题

### 3.1 FP32权重备份

这个主要是用来解决舍入误差的，一句话：**weight， activation，grad都用fp16存，只多备份一份fp32的weight**

![image-20250127222924557](https://s2.loli.net/2025/01/27/rKMgZz3GFqwDyhC.png)

可以看到，其他所有值（weights，activations， gradients）均使用 fp16 来存储，而唯独权重weights需要用 fp32 的格式额外备份一次。 这主要是因为，在更新权重的时候，往往公式: **权重 = 旧权重 + lr \* 梯度**，而在深度模型中，**lr \* 梯度** 这个值往往是非常小的，如果利用 fp16 来进行相加的话， 则很可能会出现上面所说的『舍入误差』的这个问题，导致更新无效。因此上图中，通过将weights拷贝成 fp32 格式，并且确保整个更新（update）过程是在 fp32 格式下进行的。

看到这里，可能有人提出这种 fp32 拷贝weight的方式，那岂不是使得内存占用反而更高了呢？是的， fp32 额外拷贝一份 weight 的确新增加了训练时候存储的占用。 但是实际上，在训练过程中，内存中占据大部分的基本都是 activations 的值。特别是在batchsize很大的情况下， activations 更是特别占据空间。 保存 activiations 主要是为了在 back-propogation 的时候进行计算。因此，只要 activation的值基本都是使用 fp16 来进行存储的话，则最终模型与 fp32 相比起来， 内存占用也基本能够减半

### 3.2 Loss Scale

这主要是用来解决fp16下溢问题的，也就是lr*grad往往很小，fp16压根就没办法储存，**都没走到舍入误差这个坑里呢就已经丢掉了**

做法很简单：

根据Chain Rule， loss上的scale会作用到grad上，所以可以通过loss scale平移grad

- 这样scaled-grad就可以用fp16存储了
- 更新参数前，将scaled-grad转为fp32（**fp32权重备份**），然后在逆向scale



## 4. 代码测试

当前有很多主流的训练框架，比如pytorch-lightning ，还有huggingface的Trainer，基本上不会自己从零写训练代码了

### 4.1 原生pytorch训练

虽然训练代码是原生的，仍可以借助accelerate分布式训练框架来简单改造，使用accelerate内置的混合精度配置

首先，改写原生代码

```python
from accelerate import Accelerator
#初始化
accelerator = Accelerator()

#包装model，optimizer，dataloader
model,optimizer,trainerloader,validloader = accelerator.prepare(model,optimizer,trainerloader,validloader)

#反向传播包装
accelerator.backward(loss)
```

然后，用accelerate运行

```python
#方法1
accelerator  = Accelerator(mixed_precision="bf16")

#方法2
accelerate config && choice bf 16

#方法3
accelerate launch --mixed_precision bf 16 xxx.py
```

### 4.2 Trainer训练

直接在TrainingArguments中配置bf16=true，正常运行python代码

或者，用accelerate启动，同时也要配置bf=true

```python
#方法1
accelerator  = Accelerator(mixed_precision="bf16")

#方法2
accelerate config && choice bf 16

#方法3
accelerate launch --mixed_precision bf 16 xxx.py
```

